# Review-LLM-Study

## Pipeline for LLM Fine-Tuning 
<img width="1023" alt="Screenshot 2024-12-31 at 10 24 28 AM" src="https://github.com/user-attachments/assets/c0852747-d623-43b3-8b63-f8652f05a5d6" />

## LLM Dimensions

<img width="855" alt="Screenshot 2024-12-31 at 10 26 32 AM" src="https://github.com/user-attachments/assets/0dafa35a-b6f7-4153-956b-c43603350357" />
## Model Initialization

<img width="865" alt="Screenshot 2024-12-31 at 10 42 37 AM" src="https://github.com/user-attachments/assets/e93f9e96-70b2-47c7-b9bf-69102ca8a713" />

## PEFT
<img width="893" alt="Screenshot 2024-12-31 at 10 49 33 AM" src="https://github.com/user-attachments/assets/6e194c63-c2ac-4562-920e-ea20dc5ac851" />

## Adapter for LLM

<img width="1086" alt="Screenshot 2024-12-31 at 10 56 59 AM" src="https://github.com/user-attachments/assets/cce309f4-f283-4462-9798-94f6bf437202" />

## LORA
<img width="1096" alt="Screenshot 2024-12-31 at 11 06 48 AM" src="https://github.com/user-attachments/assets/b0c8abf2-3224-4522-a0b7-ed4e337c2584" />

## QLora
![Uploading Screenshot 2024-12-31 at 11.14.29 AM.png…]()


